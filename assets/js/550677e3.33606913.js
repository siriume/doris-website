"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[29333],{15680:(e,a,t)=>{t.d(a,{xA:()=>d,yg:()=>h});var r=t(296540);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var p=r.createContext({}),s=function(e){var a=r.useContext(p),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},d=function(e){var a=s(e.components);return r.createElement(p.Provider,{value:a},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},g=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,p=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=s(t),g=n,h=c["".concat(p,".").concat(g)]||c[g]||m[g]||o;return t?r.createElement(h,i(i({ref:a},d),{},{components:t})):r.createElement(h,i({ref:a},d))}));function h(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,i=new Array(o);i[0]=g;var l={};for(var p in a)hasOwnProperty.call(a,p)&&(l[p]=a[p]);l.originalType=e,l[c]="string"==typeof e?e:n,i[1]=l;for(var s=2;s<o;s++)i[s]=t[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},294873:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var r=t(58168),n=(t(296540),t(15680));const o={title:"Lakehouse FAQ",language:"en"},i=void 0,l={unversionedId:"faq/lakehouse-faq",id:"version-2.0/faq/lakehouse-faq",title:"Lakehouse FAQ",description:"\x3c!--",source:"@site/versioned_docs/version-2.0/faq/lakehouse-faq.md",sourceDirName:"faq",slug:"/faq/lakehouse-faq",permalink:"/docs/2.0/faq/lakehouse-faq",draft:!1,tags:[],version:"2.0",frontMatter:{title:"Lakehouse FAQ",language:"en"},sidebar:"docs",previous:{title:"SQL Error",permalink:"/docs/2.0/faq/sql-faq"},next:{title:"Release 2.0.13",permalink:"/docs/2.0/releasenotes/release-2.0.13"}},p={},s=[{value:"Kerberos",id:"kerberos",level:2},{value:"JDBC Catalog",id:"jdbc-catalog",level:2},{value:"Hive Catalog",id:"hive-catalog",level:2},{value:"HDFS",id:"hdfs",level:2},{value:"DLF Catalog",id:"dlf-catalog",level:2}],d={toc:s},c="wrapper";function m(e){let{components:a,...t}=e;return(0,n.yg)(c,(0,r.A)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"kerberos"},"Kerberos"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"What to do with the ",(0,n.yg)("inlineCode",{parentName:"p"},"GSS initiate failed")," error when connecting to Hive Metastore with Kerberos authentication?"),(0,n.yg)("p",{parentName:"li"},"Usually it is caused by incorrect Kerberos authentication information, you can troubleshoot by the following steps:"),(0,n.yg)("ol",{parentName:"li"},(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"In versions before  1.2.1, the libhdfs3 library that Doris depends on does not enable gsasl. Please update to a version later than 1.2.2.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Confirm that the correct keytab and principal are set for each component, and confirm that the keytab file exists on all FE and BE nodes."),(0,n.yg)("ol",{parentName:"li"},(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"hadoop.kerberos.keytab"),"/",(0,n.yg)("inlineCode",{parentName:"li"},"hadoop.kerberos.principal"),": for Hadoop HDFS"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"hive.metastore.kerberos.principal"),": for hive metastore."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Try to replace the IP in the principal with a domain name (do not use the default ",(0,n.yg)("inlineCode",{parentName:"p"},"_HOST")," placeholder)")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Confirm that the ",(0,n.yg)("inlineCode",{parentName:"p"},"/etc/krb5.conf")," file exists on all FE and BE nodes.")))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"An error is reported when connecting to the Hive database through the Hive Catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"RemoteException: SIMPLE authentication is not enabled. Available: [TOKEN, KERBEROS]")),(0,n.yg)("p",{parentName:"li"},"If both ",(0,n.yg)("inlineCode",{parentName:"p"},"show databases")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"show tables")," are OK, and the above error occurs when querying, we need to perform the following two operations:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Core-site.xml and hdfs-site.xml need to be placed in the fe/conf and be/conf directories"),(0,n.yg)("li",{parentName:"ul"},"The BE node executes the kinit of Kerberos, restarts the BE, and then executes the query."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If an error is reported while querying the catalog with Kerberos: ",(0,n.yg)("inlineCode",{parentName:"p"},"GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos Ticket)"),"."),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Restarting FE and BE can solve the problem in most cases."),(0,n.yg)("li",{parentName:"ul"},"Before the restart all the nodes, can put ",(0,n.yg)("inlineCode",{parentName:"li"},"-Djavax.security.auth.useSubjectCredsOnly=false")," to the ",(0,n.yg)("inlineCode",{parentName:"li"},"JAVA_OPTS")," in ",(0,n.yg)("inlineCode",{parentName:"li"},'"${DORIS_HOME}/be/conf/be.conf"'),", which can obtain credentials through the underlying mechanism, rather than through the application."),(0,n.yg)("li",{parentName:"ul"},"Get more solutions to common JAAS errors from the ",(0,n.yg)("a",{parentName:"li",href:"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/Troubleshooting.html"},"JAAS Troubleshooting"),"."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"The solutions when configuring Kerberos in the catalog and encounter an error: ",(0,n.yg)("inlineCode",{parentName:"p"},"Unable to obtain password from user"),"."),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"The principal used must exist in the klist, use ",(0,n.yg)("inlineCode",{parentName:"li"},"klist -kt your.keytab")," to check."),(0,n.yg)("li",{parentName:"ul"},"Ensure the catalog configuration correct, such as missing the ",(0,n.yg)("inlineCode",{parentName:"li"},"yarn.resourcemanager.principal"),"."),(0,n.yg)("li",{parentName:"ul"},"If the preceding checks are correct, the JDK version installed by yum or other package-management utility in the current system maybe have an unsupported encryption algorithm. It is recommended to install JDK by yourself and set ",(0,n.yg)("inlineCode",{parentName:"li"},"JAVA_HOME")," environment variable."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"An error is reported when using KMS to access HDFS: ",(0,n.yg)("inlineCode",{parentName:"p"},"java.security.InvalidKeyException: Illegal key size")),(0,n.yg)("p",{parentName:"li"},"Upgrade the JDK version to a version >= Java 8 u162. Or download and install the JCE Unlimited Strength Jurisdiction Policy Files corresponding to the JDK.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If an error is reported while configuring Kerberos in the catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS]"),"."),(0,n.yg)("p",{parentName:"li"},"Need to put ",(0,n.yg)("inlineCode",{parentName:"p"},"core-site.xml")," to the ",(0,n.yg)("inlineCode",{parentName:"p"},'"${DORIS_HOME}/be/conf"')," directory."),(0,n.yg)("p",{parentName:"li"},"If an error is reported while accessing HDFS: ",(0,n.yg)("inlineCode",{parentName:"p"},"No common protection layer between client and server"),", check the ",(0,n.yg)("inlineCode",{parentName:"p"},"hadoop.rpc.protection")," on the client and server to make them consistent."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},'    <?xml version="1.0" encoding="UTF-8"?>\n    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n    \n    <configuration>\n    \n        <property>\n            <name>hadoop.security.authentication</name>\n            <value>kerberos</value>\n        </property>\n    \n    </configuration>\n'))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If an error is reported while configuring Kerberos for Broker Load: ",(0,n.yg)("inlineCode",{parentName:"p"},"Cannot locate default realm."),"."),(0,n.yg)("p",{parentName:"li"},"Add ",(0,n.yg)("inlineCode",{parentName:"p"},"-Djava.security.krb5.conf=/your-path")," to the ",(0,n.yg)("inlineCode",{parentName:"p"},"JAVA_OPTS")," of the broker startup script ",(0,n.yg)("inlineCode",{parentName:"p"},"start_broker.sh"),".")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When using Kerberos configuration in the Catalog, the ",(0,n.yg)("inlineCode",{parentName:"p"},"hadoop.username")," property cannot be appeared in Catalog properties."))),(0,n.yg)("h2",{id:"jdbc-catalog"},"JDBC Catalog"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"An error is reported when connecting to SQLServer through JDBC Catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"unable to find valid certification path to requested target")),(0,n.yg)("p",{parentName:"li"},"Please add ",(0,n.yg)("inlineCode",{parentName:"p"},"trustServerCertificate=true")," option in ",(0,n.yg)("inlineCode",{parentName:"p"},"jdbc_url"),".")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When connecting to the MySQL database through the JDBC Catalog, the Chinese characters are garbled, or the Chinese character condition query is incorrect"),(0,n.yg)("p",{parentName:"li"},"Please add ",(0,n.yg)("inlineCode",{parentName:"p"},"useUnicode=true&characterEncoding=utf-8")," in ",(0,n.yg)("inlineCode",{parentName:"p"},"jdbc_url")),(0,n.yg)("blockquote",{parentName:"li"},(0,n.yg)("p",{parentName:"blockquote"},"Note: After version 1.2.3, these parameters will be automatically added when using JDBC Catalog to connect to the MySQL database."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"An error is reported when connecting to the MySQL database through the JDBC Catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"Establishing SSL connection without server's identity verification is not recommended")),(0,n.yg)("p",{parentName:"li"},"Please add ",(0,n.yg)("inlineCode",{parentName:"p"},"useSSL=true")," in ",(0,n.yg)("inlineCode",{parentName:"p"},"jdbc_url"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When using JDBC Catalog to synchronize MySQL data to Doris, the date data synchronization error occurs. It is necessary to check whether the MySQL version corresponds to the MySQL driver package. For example, the driver com.mysql.cj.jdbc.Driver is required for MySQL8 and above."))),(0,n.yg)("h2",{id:"hive-catalog"},"Hive Catalog"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"What to do with errors such as  ",(0,n.yg)("inlineCode",{parentName:"p"},"failed to get schema")," and  ",(0,n.yg)("inlineCode",{parentName:"p"},"Storage schema reading not supported"),"  when accessing Icerberg tables via Hive Metastore?"),(0,n.yg)("p",{parentName:"li"},"To fix this, please place the Jar file package of ",(0,n.yg)("inlineCode",{parentName:"p"},"iceberg")," runtime in the ",(0,n.yg)("inlineCode",{parentName:"p"},"lib/")," directory of Hive."),(0,n.yg)("p",{parentName:"li"},"And configure as follows in  ",(0,n.yg)("inlineCode",{parentName:"p"},"hive-site.xml")," :"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"metastore.storage.schema.reader.impl=org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader\n")),(0,n.yg)("p",{parentName:"li"},"After configuring, please restart Hive Metastore.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"An error is reported when connecting Hive Catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"Caused by: java.lang.NullPointerException")),(0,n.yg)("p",{parentName:"li"},"If there is stack trace in fe.log:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"Caused by: java.lang.NullPointerException\n    at org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.getFilteredObjects(AuthorizationMetaStoreFilterHook.java:78) ~[hive-exec-3.1.3-core.jar:3.1.3]\n    at org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.filterDatabases(AuthorizationMetaStoreFilterHook.java:55) ~[hive-exec-3.1.3-core.jar:3.1.3]\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1548) ~[doris-fe.jar:3.1.3]\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1542) ~[doris-fe.jar:3.1.3]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]\n")),(0,n.yg)("p",{parentName:"li"},"Try adding ",(0,n.yg)("inlineCode",{parentName:"p"},'"metastore.filter.hook" = "org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl"')," in ",(0,n.yg)("inlineCode",{parentName:"p"},"create catalog")," statement.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If the ",(0,n.yg)("inlineCode",{parentName:"p"},"show tables")," is normal after creating the Hive Catalog, but the query report ",(0,n.yg)("inlineCode",{parentName:"p"},"java.net.UnknownHostException: xxxxx")),(0,n.yg)("p",{parentName:"li"},"Add a property in CATALOG:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"'fs.defaultFS' = 'hdfs://<your_nameservice_or_actually_HDFS_IP_and_port>'\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"The table in orc format of Hive 1.x may encounter system column names such as ",(0,n.yg)("inlineCode",{parentName:"p"},"_col0"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"_col1"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"_col2"),"... in the underlying orc file schema, which need to be specified in the catalog configuration. Add ",(0,n.yg)("inlineCode",{parentName:"p"},"hive.version")," to 1.x.x so that it will use the column names in the hive table for mapping."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-sql"},"CREATE CATALOG hive PROPERTIES (\n    'hive.version' = '1.x.x'\n);\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If an error related to the Hive Metastore is reported while querying the catalog: ",(0,n.yg)("inlineCode",{parentName:"p"},"Invalid method name"),"."),(0,n.yg)("p",{parentName:"li"},"Configure the ",(0,n.yg)("inlineCode",{parentName:"p"},"hive.version"),"."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-sql"},"CREATE CATALOG hive PROPERTIES (\n    'hive.version' = '2.x.x'\n);\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When querying a table in ORC format, FE reports an error ",(0,n.yg)("inlineCode",{parentName:"p"},"Could not obtain block")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"Caused by: java.lang.NoSuchFieldError: types")),(0,n.yg)("p",{parentName:"li"},"For ORC files, by default, FE will access HDFS to obtain file information and split files. In some cases, FE may not be able to access HDFS. It can be solved by adding the following parameters:"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"p"},'"hive.exec.orc.split.strategy" = "BI"')),(0,n.yg)("p",{parentName:"li"},"Other options: HYBRID (default), ETL.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"The values of the partition fields in the hudi table can be found on hive, but they cannot be found on doris."),(0,n.yg)("p",{parentName:"li"},"Doris and hive currently query hudi differently. Doris needs to add partition fields to the avsc file of the hudi table structure. If not added, it will cause Doris to query partition_ Val is empty (even if home. datasource. live_sync. partition_fields=partition_val is set)"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n   "type": "record",\n   "name": "record",\n   "fields": [{\n       "name": "partition_val",\n       "type": [\n           "null",\n           "string"\n           ],\n       "doc": "Preset partition field, empty string when not partitioned",\n       "default": null\n       },\n       {\n       "name": "name",\n       "type": "string",\n       "doc": "Name"\n       },\n       {\n       "name": "create_time",\n       "type": "string",\n       "doc": "Creation time"\n       }\n   ]\n}\n'))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Query the appearance of hive and encounter this error:",(0,n.yg)("inlineCode",{parentName:"p"},"java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzoCodec not found")),(0,n.yg)("p",{parentName:"li"},'Search in the hadoop environment hadoop-lzo-*.jar, and put it under "${DORIS_HOME}/fe/lib/",then restart fe.'),(0,n.yg)("p",{parentName:"li"},"Starting from version 2.0.2, this file can be placed in FE's ",(0,n.yg)("inlineCode",{parentName:"p"},"custom_lib/")," directory (if it does not exist, just create it manually) to prevent the file from being lost due to the replacement of the lib directory when upgrading the cluster.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Create a hive table specifying ",(0,n.yg)("inlineCode",{parentName:"p"},"serde")," as ",(0,n.yg)("inlineCode",{parentName:"p"},"org.apache.hadoop.hive.contrib.serde2.MultiDelimitserDe"),", and an error is reported when accessing the table: ",(0,n.yg)("inlineCode",{parentName:"p"},"storage schema reading not supported")),(0,n.yg)("p",{parentName:"li"},"Add the following configuration to the hive-site .xml file and restart the HMS service:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"}," <property>\n   <name>metastore.storage.schema.reader.impl</name>\n   <value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>\n </property> \n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty"),(0,n.yg)("p",{parentName:"li"},"Entire error info found in FE.log is shown as below:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"org.apache.doris.common.UserException: errCode = 2, detailMessage = S3 list path failed. path=s3://bucket/part-*,msg=errors while get file status listStatus on s3://bucket: com.amazonaws.SdkClientException: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\norg.apache.doris.common.UserException: errCode = 2, detailMessage = S3 list path exception. path=s3://bucket/part-*, err: errCode = 2, detailMessage = S3 list path failed. path=s3://bucket/part-*,msg=errors while get file status listStatus on s3://bucket: com.amazonaws.SdkClientException: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\norg.apache.hadoop.fs.s3a.AWSClientIOException: listStatus on s3://bucket: com.amazonaws.SdkClientException: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\nCaused by: javax.net.ssl.SSLException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\nCaused by: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\nCaused by: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty\n")),(0,n.yg)("p",{parentName:"li"},"Try to update FE node CA certificates, use command ",(0,n.yg)("inlineCode",{parentName:"p"},"update-ca-trust (CentOS/RockyLinux)"),", then restart FE process.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"BE report error: ",(0,n.yg)("inlineCode",{parentName:"p"},"java.lang.InternalError")),(0,n.yg)("p",{parentName:"li"},"If you see error in ",(0,n.yg)("inlineCode",{parentName:"p"},"be.INFO")," like:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"W20240506 15:19:57.553396 266457 jni-util.cpp:259] java.lang.InternalError\n        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.init(Native Method)\n        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.<init>(ZlibDecompressor.java:114)\n        at org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor.<init>(GzipCodec.java:229)\n        at org.apache.hadoop.io.compress.GzipCodec.createDecompressor(GzipCodec.java:188)\n        at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:183)\n        at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.<init>(CodecFactory.java:99)\n        at org.apache.parquet.hadoop.CodecFactory.createDecompressor(CodecFactory.java:223)\n        at org.apache.parquet.hadoop.CodecFactory.getDecompressor(CodecFactory.java:212)\n        at org.apache.parquet.hadoop.CodecFactory.getDecompressor(CodecFactory.java:43)\n")),(0,n.yg)("p",{parentName:"li"},"This is because the conflict between system libz.so and Doris' libz.a."),(0,n.yg)("p",{parentName:"li"},"To solve it, execute ",(0,n.yg)("inlineCode",{parentName:"p"},"export LD_LIBRARY_PATH=/path/to/be/lib:$LD_LIBRARY_PATH")," and restart BE."))),(0,n.yg)("h2",{id:"hdfs"},"HDFS"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"What to do with the",(0,n.yg)("inlineCode",{parentName:"p"},"java.lang.VerifyError: xxx")," error when accessing HDFS 3.x?"),(0,n.yg)("p",{parentName:"li"},"Doris 1.2.1 and the older versions rely on Hadoop 2.8. Please update Hadoop to 2.10.2 or update Doris to 1.2.2 or newer.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Use Hedged Read to optimize the problem of slow HDFS reading."),(0,n.yg)("p",{parentName:"li"}," In some cases, the high load of HDFS may lead to a long time to read the data on HDFS, thereby slowing down the overall query efficiency. HDFS Client provides Hedged Read.\nThis function can start another read thread to read the same data when a read request exceeds a certain threshold and is not returned, and whichever is returned first will use the result."),(0,n.yg)("p",{parentName:"li"}," This feature can be enabled in two ways:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Specify in the parameters to create the Catalog:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"```\ncreate catalog regression properties (\n    'type'='hms',\n    'hive.metastore.uris' = 'thrift://172.21.16.47:7004',\n    'dfs.client.hedged.read.threadpool.size' = '128',\n    'dfs.client.hedged.read.threshold.millis' = \"500\"\n);\n```\n\n`dfs.client.hedged.read.threadpool.size` indicates the number of threads used for Hedged Read, which are shared by one HDFS Client. Usually, for an HDFS cluster, BE nodes will share an HDFS Client.\n\n`dfs.client.hedged.read.threshold.millis` is the read threshold in milliseconds. When a read request exceeds this threshold and is not returned, Hedged Read will be triggered.\n")),(0,n.yg)("p",{parentName:"li"},"After enabling it, you can see related parameters in Query Profile:"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"p"},"TotalHedgedRead"),": The number of Hedged Reads initiated."),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"p"},"HedgedReadWins"),": The number of successful Hedged Reads (numbers initiated and returned faster than the original request)"),(0,n.yg)("p",{parentName:"li"},"Note that the value here is the cumulative value of a single HDFS Client, not the value of a single query. The same HDFS Client will be reused by multiple queries."))))),(0,n.yg)("h2",{id:"dlf-catalog"},"DLF Catalog"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When using DLF Catalog, BE reads ",(0,n.yg)("inlineCode",{parentName:"p"},"Invalid address")," when fetching JindoFS data and needs to add the domain name to IP mapping that appears in the log in ",(0,n.yg)("inlineCode",{parentName:"p"},"/ets/hosts"),".")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"When reading data is not authorized, use the ",(0,n.yg)("inlineCode",{parentName:"p"},"hadoop.username")," property to specify the authorized user.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"The metadata in the DLF Catalog is consistent with the DLF. When DLF is used to manage metadata, newly imported Hive partitions may not be synchronized by DLF, resulting in inconsistency between the DLF and Hive metadata. In this case, ensure firstly that the Hive metadata is fully synchronized by DLF."))))}m.isMDXComponent=!0}}]);